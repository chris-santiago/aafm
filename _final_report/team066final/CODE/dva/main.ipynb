{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "defined-magazine",
   "metadata": {},
   "source": [
    "# Analyzing Chilean Mutual Funds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-commission",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Most of the code required to run our analysis is located within th `dva` package, so we only need to import that and a few others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import dva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE = pathlib.Path('.').cwd()\n",
    "DATA_DIR = HERE.joinpath('data')\n",
    "RAW_FILES = DATA_DIR.joinpath('raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-checkout",
   "metadata": {},
   "source": [
    "\n",
    "## Pull Data from Web\n",
    "\n",
    "Here we connect to the AAFM Public Daily Statistics API to pull daily mutual fund data from 2015 through our project start (Feb 13, 2021).  Each JSON response is converted to a Pandas DataFrame and exported to CSV in the `RAW_FILES` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = dva.get_dates('2015-01-01', '2021-02-13')\n",
    "dva.pull_all(dates, RAW_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-nurse",
   "metadata": {},
   "source": [
    "## Convert Raw Data to Parquet\n",
    "\n",
    "We use Dask to extract and subset raw data into separate, compressed parquet files:\n",
    "\n",
    "- `fund_data.parq`, containing static, descriptive and categorical data on each unique mutual fund;\n",
    "- `fund_flows.parq`, containing time series data on mutual fund inflows and outflows, 2015 - 2021;\n",
    "- `fund_prices.parq`, containing time series data on mutual fund prices, 2015 - 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "dva.raw_to_parq(RAW_FILES, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-setup",
   "metadata": {},
   "source": [
    "## Transform and Join Data\n",
    "\n",
    "We join our static fund data with time series fund data, downsample to monthly price observations, and tranform monthly prices into monthly percentage returns.\n",
    "\n",
    "We filter out observations that have **at least** 36 months of data and still exist in the most recent month.  We'll also filter out any observations whose price does not change across the entire time period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = DATA_DIR.joinpath('fund_prices.parq')\n",
    "fund_data = DATA_DIR.joinpath('fund_data.parq')\n",
    "\n",
    "prices = dva.get_monthly_prices(price_data)\n",
    "fund_data_monthly_prices = dva.clean_from_monthly_prices_raw(dva.remove_closed_funds(dva.join_fund_data(prices, fund_data)))\n",
    "fund_data_monthly_prices.drop(['svsCategoryId', 'currency'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "static = dva.get_static_data(fund_data_monthly_prices)\n",
    "prices = dva.get_price_data(fund_data_monthly_prices)\n",
    "returns = dva.get_return_data(prices)\n",
    "dataset = dva.make_dataset(static, returns)\n",
    "cat_means = dva.get_category_means(dataset)\n",
    "monthly_returns = dva.fill_na(dataset, cat_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-celebration",
   "metadata": {},
   "source": [
    "## Map Fund Return Data to Lower Dimensions, Join with Fund Flows Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped = dva.map_all_by_group(monthly_returns.fillna(0), 'tsne')\n",
    "fund_flows = dva.get_fund_flows_2020(DATA_DIR.joinpath('fund_flows.parq'))\n",
    "fund_data_monthly_returns_tsne = mapped.merge(fund_flows, on='fundRUNSeries')\n",
    "\n",
    "sc = MinMaxScaler((10, 50))\n",
    "fund_data_monthly_returns_tsne['netPatrimony_scaled'] = sc.fit_transform(fund_data_monthly_returns_tsne['netPatrimony'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-utilization",
   "metadata": {},
   "source": [
    "## Calculate Annualized Fund Metrics, Join\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_stats = dva.add_perf_metrics(fund_data_monthly_prices)\n",
    "data = fund_data_monthly_returns_tsne.merge(fund_stats[['fundRUNSeries', 'ann_return', 'ann_stdev']], on='fundRUNSeries')\n",
    "data['ann_return'] = data['ann_return'] * 100\n",
    "data['ann_stdev'] = data['ann_stdev'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-armstrong",
   "metadata": {},
   "source": [
    "## Find Anomalies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for aafm_cat in data['aafmCategory'].unique():\n",
    "    mask = data['aafmCategory'] == aafm_cat\n",
    "    df = data[mask].copy()\n",
    "    df['dist_anomaly'] = dva.get_anomalies(df, 1.75)\n",
    "    all_data.append(df)\n",
    "scored = pd.concat(all_data).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-character",
   "metadata": {},
   "source": [
    "## Clustering with K-Means\n",
    "\n",
    "Here we'll cluster our funds for the purpose of re-creating a covariance and/or correlation matrix.  We'll start with 31 unique fund categories, and re-group into 10 unique clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-digest",
   "metadata": {},
   "source": [
    "## Selecting subset of columns\n",
    "\n",
    "We'll use the annualized returns and standard deviation, along with the TSNE components, to cluster the funds.  We're not including the monthly returns as that information is already encoded in the TSNE components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns[-4:-2] # only use annual return and stdev\n",
    "cols = cols.append(data.columns[-9:-7])  # adds tSNE coordinates\n",
    "df = data[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-concord",
   "metadata": {},
   "source": [
    "## Run K-means pipeline and plot results\n",
    "\n",
    "We'll use our original TSNE mapping, but replace the `aafmCategory` factor with our new cluster labels.\n",
    "\n",
    "### Choosing Number of Clusters\n",
    "\n",
    "We examined results of four methods to determine appropriate number of clusters:\n",
    "- Elbow method\n",
    "- Calinski-Harabasz\n",
    "- Silhouette\n",
    "- Davies-Bouldin\n",
    "\n",
    "The elbow method is rather subjective.  Results indicate the optimal number could lie between 7-10 clusters.\n",
    "\n",
    "The Calinski-Harabasz score is inconclusive-- the score exhibits multiple, increasing peaks.  The Silhouette and Davies-Bouldin scores indicate that 10 clusters may be appropriate; therefore, we choose to use 10 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline(\n",
    "        steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('cluster', KMeans(n_clusters=10, n_init=20, random_state=7))\n",
    "        ]\n",
    "    )\n",
    "pl.fit(df)\n",
    "labels = pl.named_steps['cluster'].labels_\n",
    "n_clusters = len(np.unique(labels))\n",
    "scored['cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'FundDataWithMonthlyReturnsTSNEScoredScaledClustered.csv'\n",
    "scored.to_csv(DATA_DIR.joinpath(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-audit",
   "metadata": {},
   "source": [
    "## Efficient Frontier Portfolio\n",
    "\n",
    "We'll now use generate portfolios of mutual fund assets to create efficient frontiers per Modern Portfolio Theory.  We'll first use our K-Means generated clusters and then the original AAFM categores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cluster=scored.loc[:,'2015-01-31':'2021-01-31']\n",
    "data_cluster['cluster']=scored['cluster']\n",
    "data_cluster=data_cluster.groupby('cluster').mean().T\n",
    "data_cluster=data_cluster[1:]\n",
    "data_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_funds=scored['cluster'].unique()\n",
    "\n",
    "#create 6000 portfolios so we can have a sample of almost all portfolio condition due to volatility and returns.\n",
    "np.random.seed(42)\n",
    "num_ports = 6000\n",
    "all_weights = np.zeros((num_ports, len(data_cluster.columns)))\n",
    "ret_arr = np.zeros(num_ports)\n",
    "vol_arr = np.zeros(num_ports)\n",
    "sharpe_arr = np.zeros(num_ports)\n",
    "\n",
    "for x in range(num_ports):\n",
    "    # Weights\n",
    "    weights = np.array(np.random.random(10))\n",
    "    weights = weights/np.sum(weights)\n",
    "    \n",
    "    # Save weights\n",
    "    all_weights[x,:] = weights\n",
    "    \n",
    "    # Expected return\n",
    "    ret_arr[x] = np.sum( (data_cluster.mean() * weights * 12))\n",
    "    \n",
    "    # Expected volatility\n",
    "    vol_arr[x] = np.sqrt(np.dot(weights.T, np.dot(data_cluster.cov()*12, weights)))\n",
    "    \n",
    "    # Sharpe Ratio\n",
    "    sharpe_arr[x] = ret_arr[x]/vol_arr[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the optimal portfolio is located in this point.\n",
    "print('Max sharpe ratio in the array: {}'.format(sharpe_arr.max()))\n",
    "print('Location in the array: {}'.format(sharpe_arr.argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the weight of the optimal portfolio is:\n",
    "print(all_weights[1901,:]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sr_ret=ret_arr[sharpe_arr.argmax()]\n",
    "max_sr_vol=vol_arr[sharpe_arr.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-participation",
   "metadata": {},
   "source": [
    "### Sample Portfolios Using K-Mean Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(vol_arr, ret_arr, c=sharpe_arr, cmap='viridis')\n",
    "plt.colorbar(label='Sharpe Ratio')\n",
    "plt.xlabel('Volatility')\n",
    "plt.ylabel('Return')\n",
    "plt.scatter(max_sr_vol, max_sr_ret,c='red', s=50) # red dot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_frontier_cluster=pd.DataFrame(all_weights*100, columns=cluster_funds)\n",
    "efficient_frontier_cluster['x_coord_vol']=vol_arr\n",
    "efficient_frontier_cluster['y_coord_ret']=ret_arr\n",
    "efficient_frontier_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'efficient_frontier_kmeans_cluster_withweights.csv'\n",
    "efficient_frontier_cluster.to_csv(DATA_DIR.joinpath(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-stress",
   "metadata": {},
   "source": [
    "We continue with the aafm categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aafm=data.loc[:,'2015-01-31':'2021-01-31']\n",
    "data_aafm['aafmCategory']=data['aafmCategory']\n",
    "data_aafm=data_aafm.groupby('aafmCategory').mean().T\n",
    "data_aafm=data_aafm[1:]\n",
    "data_aafm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "aafm_funds=data_aafm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 6000 portfolios so we can have a sample of almost all portfolio condition due to volatility and returns.\n",
    "np.random.seed(42)\n",
    "num_ports = 6000\n",
    "all_weights = np.zeros((num_ports, len(data_aafm.columns)))\n",
    "ret_arr = np.zeros(num_ports)\n",
    "vol_arr = np.zeros(num_ports)\n",
    "sharpe_arr = np.zeros(num_ports)\n",
    "\n",
    "for x in range(num_ports):\n",
    "    # Weights\n",
    "    weights = np.array(np.random.random(len(aafm_funds)))\n",
    "    weights = weights/np.sum(weights)\n",
    "    \n",
    "    # Save weights\n",
    "    all_weights[x,:] = weights\n",
    "    \n",
    "    # Expected return\n",
    "    ret_arr[x] = np.sum( (data_aafm.mean() * weights * 12))\n",
    "    \n",
    "    # Expected volatility\n",
    "    vol_arr[x] = np.sqrt(np.dot(weights.T, np.dot(data_aafm.cov()*12, weights)))\n",
    "    \n",
    "    # Sharpe Ratio\n",
    "    sharpe_arr[x] = ret_arr[x]/vol_arr[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the optimal portfolio is located in this point.\n",
    "print('Max sharpe ratio in the array: {}'.format(sharpe_arr.max()))\n",
    "print('Location in the array: {}'.format(sharpe_arr.argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the weight of the optimal portfolio is:\n",
    "print(all_weights[2738,:]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sr_ret=ret_arr[sharpe_arr.argmax()]\n",
    "max_sr_vol=vol_arr[sharpe_arr.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-quantity",
   "metadata": {},
   "source": [
    "### Sample Portfolios Using Original AAFM Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(vol_arr, ret_arr, c=sharpe_arr, cmap='viridis')\n",
    "plt.colorbar(label='Sharpe Ratio')\n",
    "plt.xlabel('Volatility')\n",
    "plt.ylabel('Return')\n",
    "plt.scatter(max_sr_vol, max_sr_ret,c='red', s=50) # red dot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_frontier_aafm=pd.DataFrame(all_weights*100,columns=aafm_funds)\n",
    "efficient_frontier_aafm['x_coord_vol']=vol_arr\n",
    "efficient_frontier_aafm['y_coord_ret']=ret_arr\n",
    "efficient_frontier_aafm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'efficient_frontier_kmeans_aafmcat_withweights.csv'\n",
    "efficient_frontier_cluster.to_csv(DATA_DIR.joinpath(file_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
